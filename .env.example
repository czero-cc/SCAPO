# LLM Processing Configuration
LLM_PROVIDER=openrouter  # Options: openrouter, local
LLM_PROCESSING_ENABLED=true

# OpenRouter Configuration (Recommended for cloud AI models)
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_MODEL=anthropic/claude-3-haiku  # Options: anthropic/claude-3-haiku, openai/gpt-4-turbo, meta-llama/llama-3.1-70b-instruct

# Local LLM Configuration (Alternative to OpenRouter)
LOCAL_LLM_URL=http://localhost:11434  # Ollama: http://localhost:11434, LM Studio: http://localhost:1234
LOCAL_LLM_MODEL=llama3  # Model name for Ollama (ignored by LM Studio)
LOCAL_LLM_TYPE=ollama  # Options: ollama, lmstudio

# Local LLM Context Configuration (Important for performance!)
LOCAL_LLM_MAX_CONTEXT=4096  # Maximum context tokens for your local model (e.g., 4096, 8192, 32768)
LOCAL_LLM_OPTIMAL_CHUNK=1024  # Optimal chunk size for batching (typically 1/4 of max context)

# Quality Filtering
LLM_QUALITY_THRESHOLD=0.6  # Minimum quality score for practices (0.0-1.0, higher = stricter)
# Set to 0.0 to disable quality filtering for faster processing

# Scraping Configuration
SCRAPING_INTERVAL_HOURS=6  # For scheduled/periodic scraping (not yet implemented in CLI)
MAX_POSTS_PER_SCRAPE=100
MIN_UPVOTE_RATIO=0.8  # Minimum upvote ratio for quality filtering
SCRAPING_DELAY_SECONDS=2  # Delay between scraping pages/posts (be respectful to servers)

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json
[
  {
    "tip": "Utilize GGUF format for efficient model storage and inference, enabling faster loading and reduced memory footprint. Dynamic Unslogth GGUF versions are available.",
    "confidence": 0.95,
    "source": "reddit:LocalLLaMA",
    "timestamp": "2025-08-05T13:59:06.275408",
    "example": null
  },
  {
    "tip": "Employ KV cache quantization to reduce memory requirements when using long context lengths (256k to 1M tokens) with Phi-4-14B. This significantly lowers the memory footprint per token, enabling longer context utilization. Refer to the provided GGUF files for pre-quantized versions optimized for this purpose.",
    "confidence": 0.9,
    "source": "reddit:LocalLLaMA",
    "timestamp": "2025-08-05T13:59:06.275408",
    "example": null
  },
  {
    "tip": "Utilize KV cache quantization to fit long context lengths (256k to 1M). This significantly reduces memory requirements per token.",
    "confidence": 0.95,
    "source": "reddit:LocalLLaMA",
    "timestamp": "2025-08-05T14:16:20.490398",
    "example": null
  }
]